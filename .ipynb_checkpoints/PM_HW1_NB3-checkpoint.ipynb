{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3- GLM modeling and Regularization. \n",
    "\n",
    "Using the dataset in UCO called Crimes and Communities (https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime - it has 128 variables and 1994 rows. Read this file in DIRECTLY from the URL address. Call this as Crime. You will create in Scikit learn at least 4 of these GLMs:\n",
    "-Ridge Regression\n",
    "-LASSO Regression\n",
    "-ElecticNet Regression\n",
    "-Use one other linear model found in the sci-kit learn documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pathname = 'C:/Users/email/Downloads/Predictive Modelling Misc/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving dataset from the web\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data'\n",
    "\n",
    "com = pd.read_csv(url, delimiter=',',header = None)#, names = column_names)\n",
    "\n",
    "#Exporting the csv file : dframe.to_csv(“file_name.csv”)\n",
    "#com.to_csv('C:/Users/email/OneDrive/Documents/Sandhya/Predictive Modeling/communities.csv',index = None, header=True )\n",
    "# reading AUTO.csv\n",
    "#com = pd.read_csv(pathname + \"communities.txt\",encoding='ISO-8859-1') \n",
    "#df = pd.read_csv(pathname + \"community.csv\",header = None) \n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>81440</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>95</td>\n",
       "      <td>6096</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1      2                    3    4     5     6     7     8     9    \\\n",
       "0    8   ?      ?         Lakewoodcity    1  0.19  0.33  0.02  0.90  0.12   \n",
       "1   53   ?      ?          Tukwilacity    1  0.00  0.16  0.12  0.74  0.45   \n",
       "2   24   ?      ?         Aberdeentown    1  0.00  0.42  0.49  0.56  0.17   \n",
       "3   34   5  81440  Willingborotownship    1  0.04  0.77  1.00  0.08  0.12   \n",
       "4   42  95   6096    Bethlehemtownship    1  0.01  0.55  0.02  0.95  0.09   \n",
       "\n",
       "   ...   118   119   120   121   122  123  124   125   126   127  \n",
       "0  ...  0.12  0.26  0.20  0.06  0.04  0.9  0.5  0.32  0.14  0.20  \n",
       "1  ...  0.02  0.12  0.45     ?     ?    ?    ?  0.00     ?  0.67  \n",
       "2  ...  0.01  0.21  0.02     ?     ?    ?    ?  0.00     ?  0.43  \n",
       "3  ...  0.02  0.39  0.28     ?     ?    ?    ?  0.00     ?  0.12  \n",
       "4  ...  0.04  0.09  0.02     ?     ?    ?    ?  0.00     ?  0.03  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1989</td>\n",
       "      <td>12</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>TempleTerracecity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.05</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>6</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Seasidecity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.20</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1991</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>80070</td>\n",
       "      <td>Waterburytown</td>\n",
       "      <td>10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1992</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>72600</td>\n",
       "      <td>Walthamcity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1993</td>\n",
       "      <td>6</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Ontariocity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1      2                  3    4     5     6     7     8     9    \\\n",
       "1989   12   ?      ?  TempleTerracecity   10  0.01  0.40  0.10  0.87  0.12   \n",
       "1990    6   ?      ?        Seasidecity   10  0.05  0.96  0.46  0.28  0.83   \n",
       "1991    9   9  80070      Waterburytown   10  0.16  0.37  0.25  0.69  0.04   \n",
       "1992   25  17  72600        Walthamcity   10  0.08  0.51  0.06  0.87  0.22   \n",
       "1993    6   ?      ?        Ontariocity   10  0.20  0.78  0.14  0.46  0.24   \n",
       "\n",
       "      ...   118   119   120   121   122   123  124   125   126   127  \n",
       "1989  ...  0.01  0.28  0.05     ?     ?     ?    ?  0.00     ?  0.09  \n",
       "1990  ...  0.02  0.37  0.20     ?     ?     ?    ?  0.00     ?  0.45  \n",
       "1991  ...  0.08  0.32  0.18  0.08  0.06  0.78    0  0.91  0.28  0.23  \n",
       "1992  ...  0.03  0.38  0.33  0.02  0.02  0.79    0  0.22  0.18  0.19  \n",
       "1993  ...  0.11  0.30  0.05  0.08  0.04  0.73  0.5  1.00  0.13  0.48  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all these columns to be drop as 5 of these attributes are not counted as predictive as per the dataset maker\n",
    "# and  I don't count rest 23 attributes as predictive as to be included in my work.\n",
    "\n",
    "column_list = list(range(0,5)) +[30]+ list(range(101,118)) + list(range(121,125)) + [126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 30,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 126]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the whole attribut list\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following command works only when I used header=None, if header is left unhandled \n",
    "# it raises an error \"columns are not on axis=1\" \n",
    "com=com.drop(column_list, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas\n",
    "#  supplying a list to the set_axis method that is equal in length to the number of columns\n",
    "# using set_axis with a list and inplace=False\n",
    "com = com.set_axis(['population','householdsize','racepctblack','racePctWhite',\n",
    "             'racePctAsian','racePctHisp','agePct12t21','agePct12t29','agePct16t24','agePct65up','numbUrban','pctUrban',\n",
    "             'medIncome','pctWWage','pctWFarmSelf','pctWInvInc','pctWSocSec','pctWPubAsst','pctWRetire','medFamInc',\n",
    "             'perCapInc','whitePerCap','blackPerCap','indianPerCap','AsianPerCap','HispPerCap','NumUnderPov',\n",
    "            'PctPopUnderPov','PctLess9thGrade','PctNotHSGrad','PctBSorMore','PctUnemployed','PctEmploy',\n",
    "            'PctEmplManu','PctEmplProfServ','PctOccupManu','PctOccupMgmtProf','MalePctDivorce','MalePctNevMarr',\n",
    "            'FemalePctDiv','TotalPctDiv','PersPerFam','PctFam2Par','PctKids2Par','PctYoungKids2Par','PctTeen2Par',\n",
    "            'PctWorkMomYoungKids','PctWorkMom','NumIlleg','PctIlleg','NumImmig','PctImmigRecent','PctImmigRec5',\n",
    "            'PctImmigRec10','PctImmigRec10','PctRecentImmig','PctRecImmig5','PctRecImmig8','PctRecImmig10','PctSpeakEnglOnly',\n",
    "            'PctNotSpeakEnglWell','PctLargHouseFam','PctLargHouseOccup','PersPerOccupHous','PersPerOwnOccHous',\n",
    "            'PersPerRentOccHous','PctPersOwnOccup','PctPersDenseHous','PctHousLess3BR','MedNumBR','HousVacant',\n",
    "            'PctHousOccup','PctHousOwnOcc','PctVacantBoarded','PctVacMore6Mos','MedYrHousBuilt','PctHousNoPhone',\n",
    "            'PctWOFullPlumb','OwnOccLowQuart','OwnOccMedVal','OwnOccHiQuart','RentLowQ','RentMedian','RentHighQ',\n",
    "            'MedRent','MedRentPctHousInc','MedOwnCostPctInc','MedOwnCostPctIncNoMtg','NumInShelters','NumStreet',\n",
    "            'PctForeignBorn','PctBornSameState','PctSameHouse85','PctSameCity85','PctSameState85','LandArea',\n",
    "             'PopDens',' PctUsePubTrans','LemasPctOfficDrugUn','ViolentCrimesPerPop'], axis='columns', inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1994, 100)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "population               float64\n",
       "householdsize            float64\n",
       "racepctblack             float64\n",
       "racePctWhite             float64\n",
       "racePctAsian             float64\n",
       "racePctHisp              float64\n",
       "agePct12t21              float64\n",
       "agePct12t29              float64\n",
       "agePct16t24              float64\n",
       "agePct65up               float64\n",
       "numbUrban                float64\n",
       "pctUrban                 float64\n",
       "medIncome                float64\n",
       "pctWWage                 float64\n",
       "pctWFarmSelf             float64\n",
       "pctWInvInc               float64\n",
       "pctWSocSec               float64\n",
       "pctWPubAsst              float64\n",
       "pctWRetire               float64\n",
       "medFamInc                float64\n",
       "perCapInc                float64\n",
       "whitePerCap              float64\n",
       "blackPerCap              float64\n",
       "indianPerCap             float64\n",
       "AsianPerCap              float64\n",
       "HispPerCap               float64\n",
       "NumUnderPov              float64\n",
       "PctPopUnderPov           float64\n",
       "PctLess9thGrade          float64\n",
       "PctNotHSGrad             float64\n",
       "                          ...   \n",
       "HousVacant               float64\n",
       "PctHousOccup             float64\n",
       "PctHousOwnOcc            float64\n",
       "PctVacantBoarded         float64\n",
       "PctVacMore6Mos           float64\n",
       "MedYrHousBuilt           float64\n",
       "PctHousNoPhone           float64\n",
       "PctWOFullPlumb           float64\n",
       "OwnOccLowQuart           float64\n",
       "OwnOccMedVal             float64\n",
       "OwnOccHiQuart            float64\n",
       "RentLowQ                 float64\n",
       "RentMedian               float64\n",
       "RentHighQ                float64\n",
       "MedRent                  float64\n",
       "MedRentPctHousInc        float64\n",
       "MedOwnCostPctInc         float64\n",
       "MedOwnCostPctIncNoMtg    float64\n",
       "NumInShelters            float64\n",
       "NumStreet                float64\n",
       "PctForeignBorn           float64\n",
       "PctBornSameState         float64\n",
       "PctSameHouse85           float64\n",
       "PctSameCity85            float64\n",
       "PctSameState85           float64\n",
       "LandArea                 float64\n",
       "PopDens                  float64\n",
       " PctUsePubTrans          float64\n",
       "LemasPctOfficDrugUn      float64\n",
       "ViolentCrimesPerPop      float64\n",
       "Length: 100, dtype: object"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all columnswith their data type in the data frame\n",
    "com.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is checking any object or categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any categorical variable\n",
    "obj_df = com.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# dropping the empty rows at file-end\n",
    "#obj_df.dropna(how=\"all\", inplace=True) \n",
    "obj_df[:3]\n",
    "# no categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My data set \"communities crimes (com) analysis:\n",
    "1. Dataset description: The set has 1994 observations with 100 variables. 99 variables are independent variables predicting violent crimes per population.\n",
    "2. Objective: A supervised learning to learn the relationship between the the independent variables such as \"different population for community(native americans,caucasians,hispanic,asian etc.), their respective mean household size, their respective median household income, their respective per capita income, number of people under poverty level, mean persons per houhold etc.\" and the dependent variable which is the violent crimes per population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.057593</td>\n",
       "      <td>0.463395</td>\n",
       "      <td>0.179629</td>\n",
       "      <td>0.753716</td>\n",
       "      <td>0.153681</td>\n",
       "      <td>0.144022</td>\n",
       "      <td>0.424218</td>\n",
       "      <td>0.493867</td>\n",
       "      <td>0.336264</td>\n",
       "      <td>0.423164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215552</td>\n",
       "      <td>0.608892</td>\n",
       "      <td>0.535050</td>\n",
       "      <td>0.626424</td>\n",
       "      <td>0.651530</td>\n",
       "      <td>0.065231</td>\n",
       "      <td>0.232854</td>\n",
       "      <td>0.161685</td>\n",
       "      <td>0.094052</td>\n",
       "      <td>0.237979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.126906</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.253442</td>\n",
       "      <td>0.244039</td>\n",
       "      <td>0.208877</td>\n",
       "      <td>0.232492</td>\n",
       "      <td>0.155196</td>\n",
       "      <td>0.143564</td>\n",
       "      <td>0.166505</td>\n",
       "      <td>0.179185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231134</td>\n",
       "      <td>0.204329</td>\n",
       "      <td>0.181352</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.198221</td>\n",
       "      <td>0.109459</td>\n",
       "      <td>0.203092</td>\n",
       "      <td>0.229055</td>\n",
       "      <td>0.240328</td>\n",
       "      <td>0.232985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "count  1994.000000    1994.000000   1994.000000   1994.000000   1994.000000   \n",
       "mean      0.057593       0.463395      0.179629      0.753716      0.153681   \n",
       "std       0.126906       0.163717      0.253442      0.244039      0.208877   \n",
       "min       0.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "25%       0.010000       0.350000      0.020000      0.630000      0.040000   \n",
       "50%       0.020000       0.440000      0.060000      0.850000      0.070000   \n",
       "75%       0.050000       0.540000      0.230000      0.940000      0.170000   \n",
       "max       1.000000       1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       racePctHisp  agePct12t21  agePct12t29  agePct16t24   agePct65up  ...  \\\n",
       "count  1994.000000  1994.000000  1994.000000  1994.000000  1994.000000  ...   \n",
       "mean      0.144022     0.424218     0.493867     0.336264     0.423164  ...   \n",
       "std       0.232492     0.155196     0.143564     0.166505     0.179185  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.010000     0.340000     0.410000     0.250000     0.300000  ...   \n",
       "50%       0.040000     0.400000     0.480000     0.290000     0.420000  ...   \n",
       "75%       0.160000     0.470000     0.540000     0.360000     0.530000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "       PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "count     1994.000000       1994.000000     1994.000000    1994.000000   \n",
       "mean         0.215552          0.608892        0.535050       0.626424   \n",
       "std          0.231134          0.204329        0.181352       0.200521   \n",
       "min          0.000000          0.000000        0.000000       0.000000   \n",
       "25%          0.060000          0.470000        0.420000       0.520000   \n",
       "50%          0.130000          0.630000        0.540000       0.670000   \n",
       "75%          0.280000          0.777500        0.660000       0.770000   \n",
       "max          1.000000          1.000000        1.000000       1.000000   \n",
       "\n",
       "       PctSameState85     LandArea      PopDens   PctUsePubTrans  \\\n",
       "count     1994.000000  1994.000000  1994.000000      1994.000000   \n",
       "mean         0.651530     0.065231     0.232854         0.161685   \n",
       "std          0.198221     0.109459     0.203092         0.229055   \n",
       "min          0.000000     0.000000     0.000000         0.000000   \n",
       "25%          0.560000     0.020000     0.100000         0.020000   \n",
       "50%          0.700000     0.040000     0.170000         0.070000   \n",
       "75%          0.790000     0.070000     0.280000         0.190000   \n",
       "max          1.000000     1.000000     1.000000         1.000000   \n",
       "\n",
       "       LemasPctOfficDrugUn  ViolentCrimesPerPop  \n",
       "count          1994.000000          1994.000000  \n",
       "mean              0.094052             0.237979  \n",
       "std               0.240328             0.232985  \n",
       "min               0.000000             0.000000  \n",
       "25%               0.000000             0.070000  \n",
       "50%               0.000000             0.150000  \n",
       "75%               0.000000             0.330000  \n",
       "max               1.000000             1.000000  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics for all columns\n",
    "com.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing columns with missing values\n",
    "nulls = com.isnull().sum()\n",
    "nulls[nulls > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking any missing value in the dataset\n",
    "#com.describe()\n",
    "#com\n",
    "#com.isnull()\n",
    "com.isnull().values.any()\n",
    "# no NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#com.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>...</th>\n",
       "      <th>NumStreet</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ViolentCrimesPerPop  population  householdsize  racepctblack  racePctWhite  \\\n",
       "0                 0.20        0.19           0.33          0.02          0.90   \n",
       "1                 0.67        0.00           0.16          0.12          0.74   \n",
       "2                 0.43        0.00           0.42          0.49          0.56   \n",
       "3                 0.12        0.04           0.77          1.00          0.08   \n",
       "4                 0.03        0.01           0.55          0.02          0.95   \n",
       "5                 0.14        0.02           0.28          0.06          0.54   \n",
       "6                 0.03        0.01           0.39          0.00          0.98   \n",
       "7                 0.55        0.01           0.74          0.03          0.46   \n",
       "8                 0.53        0.03           0.34          0.20          0.84   \n",
       "9                 0.15        0.01           0.40          0.06          0.87   \n",
       "\n",
       "   racePctAsian  racePctHisp  agePct12t21  agePct12t29  agePct16t24  ...  \\\n",
       "0          0.12         0.17         0.34         0.47         0.29  ...   \n",
       "1          0.45         0.07         0.26         0.59         0.35  ...   \n",
       "2          0.17         0.04         0.39         0.47         0.28  ...   \n",
       "3          0.12         0.10         0.51         0.50         0.34  ...   \n",
       "4          0.09         0.05         0.38         0.38         0.23  ...   \n",
       "5          1.00         0.25         0.31         0.48         0.27  ...   \n",
       "6          0.06         0.02         0.30         0.37         0.23  ...   \n",
       "7          0.20         1.00         0.52         0.55         0.36  ...   \n",
       "8          0.02         0.00         0.38         0.45         0.28  ...   \n",
       "9          0.30         0.03         0.90         0.82         0.80  ...   \n",
       "\n",
       "   NumStreet  PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "0        0.0            0.12              0.42            0.50           0.51   \n",
       "1        0.0            0.21              0.50            0.34           0.60   \n",
       "2        0.0            0.14              0.49            0.54           0.67   \n",
       "3        0.0            0.19              0.30            0.73           0.64   \n",
       "4        0.0            0.11              0.72            0.64           0.61   \n",
       "5        0.0            0.70              0.42            0.49           0.73   \n",
       "6        0.0            0.15              0.81            0.77           0.91   \n",
       "7        0.0            0.59              0.58            0.52           0.79   \n",
       "8        0.0            0.01              0.78            0.48           0.79   \n",
       "9        0.0            0.22              0.42            0.34           0.23   \n",
       "\n",
       "   PctSameState85  LandArea  PopDens   PctUsePubTrans  LemasPctOfficDrugUn  \n",
       "0            0.64      0.12     0.26             0.20                 0.32  \n",
       "1            0.52      0.02     0.12             0.45                 0.00  \n",
       "2            0.56      0.01     0.21             0.02                 0.00  \n",
       "3            0.65      0.02     0.39             0.28                 0.00  \n",
       "4            0.53      0.04     0.09             0.02                 0.00  \n",
       "5            0.64      0.01     0.58             0.10                 0.00  \n",
       "6            0.84      0.05     0.08             0.06                 0.00  \n",
       "7            0.78      0.01     0.33             0.00                 0.00  \n",
       "8            0.75      0.04     0.17             0.04                 0.00  \n",
       "9            0.09      0.00     0.47             0.11                 0.00  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# designating target variable name\n",
    "targetName = 'ViolentCrimesPerPop'\n",
    "targetSeries = com[targetName]\n",
    "#remove target from current location and insert in column 0\n",
    "del com[targetName]\n",
    "com.insert(0, targetName, targetSeries)\n",
    "#targetSeries #new target \n",
    "#reprint dataframe and see target is in position 0\n",
    "com.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA on target variable\n",
    "n, bins, patches = plt.hist(com.ViolentCrimesPerPop)#, facecolor='blue', alpha=0.75)\n",
    "#plt.ylim(0, 15)\n",
    "plt.xlabel(\"Crimes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title('Histogram of Violent Crimes Per Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1994.000000\n",
       "mean        0.237979\n",
       "std         0.232985\n",
       "min         0.000000\n",
       "25%         0.070000\n",
       "50%         0.150000\n",
       "75%         0.330000\n",
       "max         1.000000\n",
       "Name: ViolentCrimesPerPop, dtype: float64"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com['ViolentCrimesPerPop'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['population', 'householdsize', 'racepctblack', 'racePctWhite',\n",
       "       'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29',\n",
       "       'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome',\n",
       "       'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst',\n",
       "       'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap',\n",
       "       'indianPerCap', 'AsianPerCap', 'HispPerCap', 'NumUnderPov',\n",
       "       'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore',\n",
       "       'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ',\n",
       "       'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr',\n",
       "       'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par',\n",
       "       'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids',\n",
       "       'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent',\n",
       "       'PctImmigRec5', 'PctImmigRec10', 'PctImmigRec10', 'PctRecentImmig',\n",
       "       'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly',\n",
       "       'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup',\n",
       "       'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous',\n",
       "       'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR',\n",
       "       'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded',\n",
       "       'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb',\n",
       "       'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ',\n",
       "       'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc',\n",
       "       'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters',\n",
       "       'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85',\n",
       "       'PctSameCity85', 'PctSameState85', 'LandArea', 'PopDens',\n",
       "       ' PctUsePubTrans', 'LemasPctOfficDrugUn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# designating features data\n",
    "com.feature_names = com.columns[1:]\n",
    "#com.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is representing for independent variables(features) and Y is representing for dependent variable(target)\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split dataset into testing and training\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    com.iloc[:,1:].values, com.iloc[:,0].values, test_size=0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599 599 1395 1395\n"
     ]
    }
   ],
   "source": [
    "# length of the objects\n",
    "print(len(X_test), len(Y_test),len(X_train),len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1395, 99)\n",
      "(599, 99)\n",
      "(1395,)\n",
      "(599,)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the objects\n",
    "print(X_train.shape) # observations & features\n",
    "print(X_test.shape)\n",
    "\n",
    "print(Y_train.shape) # observations & target\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ridge Regression\n",
    "So ridge regression puts constraint on the coefficients (w). The penalty term (lambda) regularizes the coefficients such that if the coefficients take large values the optimization function is penalized. So, ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "0.016467265317273057\n",
      "0.6946736915641112\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Ridge\n",
    "# load the diabetes datasets\n",
    "#dataset = datasets.load_diabetes()\n",
    "# fit a ridge regression model to the data\n",
    "model = Ridge(alpha=0.1)\n",
    "model.fit(X_train, Y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = Y_train\n",
    "predicted = model.predict(X_train)\n",
    "# summarize the fit of the model\n",
    "mse = np.mean((predicted-expected)**2)\n",
    "print(mse)\n",
    "print(model.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection. Just like Ridge regression the regularization parameter (lambda) can be controlled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "0.05393333251114451\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "# fit a LASSO model to the data\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X_train, Y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = Y_train\n",
    "predicted = model.predict(X_train)\n",
    "# summarize the fit of the model\n",
    "mse = np.mean((predicted-expected)**2)\n",
    "print(mse)\n",
    "print(model.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "0.05393333251114451\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "# fit a model to the data\n",
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet(alpha=0.1)\n",
    "model.fit(X_train, Y_train)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = Y_train\n",
    "predicted = model.predict(X_train)\n",
    "# summarize the fit of the model\n",
    "mse = np.mean((predicted-expected)**2)\n",
    "print(mse)\n",
    "print(model.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating Violent Crimes per population using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0 0.36549914919807397 1.0 0.0 0.24002867383512547\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_train), np.min(X_train), np.mean(X_train), np.max(Y_train), np.min(Y_train), np.mean(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98, 0.64, 0.58, ..., 0.1 , 0.91, 0.95])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_new\n",
    "X_new[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x000001F9D8D9C3C8>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import *\n",
    "fs=SelectKBest(score_func=f_regression,k=5)\n",
    "X_new=fs.fit_transform(X_train,Y_train)\n",
    "print(zip(fs.get_support(),com.feature_names))\n",
    "#print(X_new.scores_)\n",
    "x_min, x_max = X_new[:,0].min() - .5, X_new[:, 0].max() + .5\n",
    "y_min, y_max = Y_train.min() - .5, Y_train.max() + .5\n",
    "#fig=plt.figure()\n",
    "#fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# Two subplots, unpack the axes array immediately\n",
    "fig, axes = plt.subplots(1,5)\n",
    "fig.set_size_inches(12,12)\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].set_aspect('equal')\n",
    "    axes[i].set_title('Feature ' + str(i))\n",
    "    axes[i].set_xlabel('Feature')\n",
    "    axes[i].set_ylabel('Violent Crimes Per Population')\n",
    "    axes[i].set_xlim(x_min, x_max)\n",
    "    axes[i].set_ylim(y_min, y_max)\n",
    "    plt.sca(axes[i])\n",
    "    plt.scatter(X_new[:,i],Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.870544309124465 -3.7371761824783802 -2.773125770788125e-16 1.0 0.0 0.24002867383512547\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "#scalerX = StandardScaler().fit(X_train)\n",
    "#scalery = StandardScaler().fit(y_train)\n",
    "\n",
    "#X_train = scalerX.transform(X_train)\n",
    "#y_train = StandardScaler().fit_transform((y_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "#y_test = StandardScaler().fit_transform(y_test)\n",
    "\n",
    "print(np.max(X_train), np.min(X_train), np.mean(X_train), np.max(Y_train), np.min(Y_train), np.mean(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a lineal model, SGDRegressor, that tries to find the hyperplane that minimizes a certain loss function (typically, the sum of squared distances from each instance to the hyperplane). It uses Stochastic Gradient Descent to find the minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "def train_and_evaluate(clf, X_train, Y_train):\n",
    "    \n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    print(\"Coefficient of determination on training set:\",clf.score(X_train, Y_train))\n",
    "    \n",
    "    # create a k-fold croos validation iterator of k=5 folds\n",
    "    scores = cross_val_score(clf, X_train, Y_train, cv=5)\n",
    "    print(\"Average coefficient of determination using 5-fold crossvalidation:\",np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
      "       eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
      "       n_iter=None, n_iter_no_change=5, penalty=None, power_t=0.25,\n",
      "       random_state=42, shuffle=False, tol=None, validation_fraction=0.1,\n",
      "       verbose=0, warm_start=False)\n",
      "Coefficient of determination on training set: 0.6596012277483991\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.5994797830074367\n",
      "[-7.12748093e-03  7.85510981e-03  4.29404515e-02 -3.17230988e-02\n",
      " -1.08441718e-02  5.27117483e-03  2.32140159e-03 -1.49850993e-02\n",
      " -8.69724486e-04  4.97111383e-03 -5.34151616e-03  1.14304358e-02\n",
      "  2.71023033e-03 -1.70004890e-02  9.91771012e-03 -1.81145619e-02\n",
      "  1.04083192e-02  8.28061355e-03 -1.10779566e-02  4.93348121e-03\n",
      " -3.10351556e-03 -1.28603997e-03  1.70687604e-04 -6.22710403e-03\n",
      "  8.94455224e-03  4.69684682e-03 -6.88416731e-06 -1.14589334e-02\n",
      " -1.55221932e-02  5.27210545e-03 -1.99968348e-03 -1.03783551e-02\n",
      "  6.18342137e-03 -1.76459587e-03 -2.43852811e-03  6.88494542e-03\n",
      " -5.57489114e-03  9.62196161e-03  9.92083396e-03  2.08377405e-03\n",
      "  4.22283998e-03  8.17747941e-03 -1.92674897e-02 -2.62359434e-02\n",
      " -7.38820633e-03 -4.39475338e-03  4.54827316e-03 -1.78742918e-02\n",
      " -9.06074378e-03  4.17871970e-02 -1.55693139e-02 -4.42194961e-03\n",
      "  4.17800149e-03 -1.87399444e-03  3.26811666e-03 -9.64778412e-04\n",
      "  6.65579014e-04  5.26307051e-03  7.06794278e-03  8.76339402e-04\n",
      " -1.29198447e-02 -8.19599660e-04 -1.93801816e-03  9.04640767e-03\n",
      "  1.34400101e-03 -1.29560832e-03 -5.88403582e-03  1.16431271e-02\n",
      "  6.92427541e-03 -2.14618838e-03  1.80173855e-02 -1.26543237e-02\n",
      "  6.51404078e-04  1.50588109e-02 -4.46190370e-03 -6.78812589e-03\n",
      "  2.70052273e-03 -7.26830020e-05 -3.07865288e-03 -2.65892818e-03\n",
      " -6.08212572e-03 -1.62129762e-02  7.50766456e-03  7.55646700e-03\n",
      "  1.46150434e-02  6.53908665e-03 -2.70579301e-03 -1.99527615e-02\n",
      "  1.44211404e-02  1.66669292e-02  1.18988498e-02  2.42236422e-04\n",
      "  3.39178347e-03  8.89607989e-03  8.13765294e-04  5.55050175e-03\n",
      "  2.54609819e-03 -6.12128592e-03  7.38279265e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf_sgd = linear_model.SGDRegressor(loss='squared_loss', penalty=None,  random_state=42, shuffle=False)\n",
    "print(clf_sgd)\n",
    "train_and_evaluate(clf_sgd,X_train,Y_train)\n",
    "print(clf_sgd.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.63117879922244\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.6292508212442751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_sgd1 = linear_model.SGDRegressor(loss='squared_loss', penalty='l2',  random_state=42)\n",
    "train_and_evaluate(clf_sgd1,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.6425712638930838\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.630332123423951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_sgd2 = linear_model.SGDRegressor(loss='squared_loss', penalty='l1',  random_state=42)\n",
    "train_and_evaluate(clf_sgd2,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.633199109863984\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.6294308640950836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_sgd3 = linear_model.SGDRegressor(loss='squared_loss', penalty='elasticnet',  random_state=42)\n",
    "train_and_evaluate(clf_sgd3,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.6951318097684095\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.6300629882810103\n"
     ]
    }
   ],
   "source": [
    "clf_ridge = linear_model.Ridge()\n",
    "train_and_evaluate(clf_ridge,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.6856356496551095\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.6190012317502703\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf_svr= svm.SVR(kernel='linear')\n",
    "train_and_evaluate(clf_svr,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.8303947613930192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coefficient of determination using 5-fold crossvalidation: 0.4250685436382078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_svr_poly= svm.SVR(kernel='poly')\n",
    "train_and_evaluate(clf_svr_poly,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.8450691127315662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coefficient of determination using 5-fold crossvalidation: 0.5760515510212867\n"
     ]
    }
   ],
   "source": [
    "clf_svr_rbf= svm.SVR(kernel='rbf')\n",
    "train_and_evaluate(clf_svr_rbf,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.7701335036990862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coefficient of determination using 5-fold crossvalidation: 0.46008465585291203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandhya\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_svr_poly2= svm.SVR(kernel='poly',degree=2)\n",
    "train_and_evaluate(clf_svr_poly2,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try again Random Forests, in their Extra Trees, and Regression version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 1.0\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.5930832858854332\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "clf_et=ensemble.ExtraTreesRegressor(n_estimators=10,random_state=42)\n",
    "train_and_evaluate(clf_et,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00337857 0.00204942 0.08949425 0.10204598 0.00362212 0.0119845\n",
      " 0.00265069 0.00183876 0.00234999 0.00183369 0.0029872  0.01310641\n",
      " 0.00248015 0.00286359 0.00394971 0.00387714 0.00238104 0.00559263\n",
      " 0.0028654  0.00339447 0.00257536 0.00173342 0.00584633 0.00383411\n",
      " 0.00518163 0.00461355 0.00408139 0.00316356 0.00283516 0.00411503\n",
      " 0.0015925  0.00340651 0.00326984 0.00690025 0.00450057 0.00478574\n",
      " 0.00330967 0.00799568 0.002973   0.02225188 0.00554084 0.00265576\n",
      " 0.04344271 0.14890639 0.0287061  0.03139753 0.00406179 0.00341938\n",
      " 0.0129646  0.12703607 0.00317011 0.0040651  0.00391583 0.002718\n",
      " 0.00427517 0.00121798 0.00320327 0.00331768 0.00195365 0.00229324\n",
      " 0.01209774 0.01355145 0.00525011 0.00226233 0.00302123 0.00272479\n",
      " 0.00485836 0.01399003 0.00745458 0.00246476 0.00976377 0.0076859\n",
      " 0.0039329  0.00933977 0.00272479 0.00513809 0.00338499 0.00719758\n",
      " 0.00207077 0.00448119 0.00109242 0.00323806 0.00233195 0.00110953\n",
      " 0.0023551  0.00596735 0.00400141 0.00355911 0.00494973 0.00464676\n",
      " 0.00314914 0.00386815 0.00693974 0.00348179 0.0037994  0.00240578\n",
      " 0.00389592 0.01174109 0.00210038] Index(['population', 'householdsize', 'racepctblack', 'racePctWhite',\n",
      "       'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29',\n",
      "       'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome',\n",
      "       'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst',\n",
      "       'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap',\n",
      "       'indianPerCap', 'AsianPerCap', 'HispPerCap', 'NumUnderPov',\n",
      "       'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore',\n",
      "       'PctUnemployed', 'PctEmploy', 'PctEmplManu', 'PctEmplProfServ',\n",
      "       'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr',\n",
      "       'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par',\n",
      "       'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids',\n",
      "       'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent',\n",
      "       'PctImmigRec5', 'PctImmigRec10', 'PctImmigRec10', 'PctRecentImmig',\n",
      "       'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', 'PctSpeakEnglOnly',\n",
      "       'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup',\n",
      "       'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous',\n",
      "       'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR',\n",
      "       'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded',\n",
      "       'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb',\n",
      "       'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'RentLowQ',\n",
      "       'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc',\n",
      "       'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters',\n",
      "       'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85',\n",
      "       'PctSameCity85', 'PctSameState85', 'LandArea', 'PopDens',\n",
      "       ' PctUsePubTrans', 'LemasPctOfficDrugUn'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(clf_et.feature_importances_,com.feature_names)#com.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, evaluate our classifiers on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination:0.620 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(X,Y,clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True, show_r2_score=False):\n",
    "    y_pred=clf.predict(X)   \n",
    "    if show_accuracy:\n",
    "        print(\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(Y,y_pred)),\"\\n\")\n",
    "\n",
    "    if show_classification_report:\n",
    "        print(\"Classification report\")\n",
    "        print(metrics.classification_report(Y,y_pred),\"\\n\")\n",
    "        \n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion matrix\")\n",
    "        print(metrics.confusion_matrix(Y,y_pred),\"\\n\")\n",
    "        \n",
    "    if show_r2_score:\n",
    "        print(\"Coefficient of determination:{0:.3f}\".format(metrics.r2_score(Y,y_pred)),\"\\n\")\n",
    "\n",
    "        \n",
    "measure_performance(X_test,Y_test,clf_et, show_accuracy=False, show_classification_report=False,show_confusion_matrix=False, show_r2_score=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        The End:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
